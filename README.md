# Traffic Light Deep Learning Detection using the Raspberry PI, Tensorflow and OpenCV

 <br></br>
 
## Requirements

### Hardware Components
<ol>
<li>Raspberry PI 4</li>
<li>RC Car Frame with 3 DC Motors-FWD, RWD, and turning</li>
<li>2 L298N Modules</li>
<li>HC-SR04 Ultrasonic Sensor</li>
<li>PI Camera</li>
<li>5V/2.1A Power Bank</li>
</ol>

### Major Tools/Libraries
<ol>
<li>Rpi GPIO</li>
<li>Open CV</li>
<li>Tensorflow/Tensorflow Lite</li>
  <li><a href="https://app.roboflow.com/project/traffic-lights-cchqa/14">Roboflow</a></li>
</ol>

 <br></br>

## Steps

### Problem:
<p>Traffic Light Detection and using Computer Vision to start moving</p>

<br>

### Building the Traffic Light

<p>Circuit Diagram:</p>

![screen-recorder-wed-jul-21-2021-09-08-45](https://user-images.githubusercontent.com/49088702/126446840-2f38d5bb-9d72-4761-898b-e4c64a63b572.gif)

<br>

### Data Collection

<p>Data Collection was done using a webcam</p>
<p>Total Images Collected: 580</p>
<p>Image Preprocessing was done in <a href="https://app.roboflow.com/project/traffic-lights-cchqa/14">Roboflow</a></p>
<p> Total Images for Training: 1500</p>

<br>
  
### Training

<p> Model.py creates downloads all dependencies and creates a pipeline.config file that uses .tfrecord files generated by Roboflow </p>
<p> DynamicDetection.py tests the model with a webcam </p>
<p> Pretrained model used: SSD MobileNet V2 FPNLite 320x320 from <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md">TF Model Zoo</a></p>
<p> The best performance was obtained after training for 2000 steps which is ckpt-2 </p>
<p> CkptSave.sh is a script that copies checkpoints from training and moves them to another folder whenever they are generated </p>

<br></br>

### Hardware Control

<p> MotorControl.py controls the speed and direction of all three motors independenlty </p>
<p> It turns left and right using the left and right arrow keys </p>
<p> Up and Down arrow keys are used to increase and decrease the cycle of the PWM signal to the front and back motors </p>
<p> Pressing the 'f' key moves the car forward, pressing 'r' key makes it move in reverse </p>
<p> Keys 'a' and 's' control the servo where the camera is mounted </p>
<p> Key 'd' measures the distance in front of the car using the HC-SR04 Ultrasonic Sensor </p>
 
<br></br>

### Bash commands

<ol>
  <li><b>Start Training:</b>
    <br></br>

    CUDA_VISIBLE_DEVICES="0" python3 models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path='pipeline_file.config' \
    --model_dir='training/' \
    --alsologtostderr \
    --num_train_steps=5000 \
    --sample_1_of_n_eval_examples=1 \
    --num_eval_steps=100
  
    
</li>

<li><b>Start Evaluating:</b>
  <p><b>NOTE: CUDA_VISIBLE_DEVICES="-1" hides the GPU so that evaluation doesn't take away GPU resources from the training</b></p>
   

    CUDA_VISIBLE_DEVICES="-1" python3 models/research/object_detection/model_main_tf2.py \
    --pipeline_config_path='pipeline_file.config' \
    --model_dir='training/' \
    --checkpoint_dir=training/ \
    --eval_dir=eval/
    
    
</li>

  <li><b>Track GPU usage:</b>
      <br></br>
    
    nvidia-smi -l 1
  

    
</li>


  <li><b>Track Training and Evaluation with Tensorboard:</b>
      <br></br>
    
    tensorboard --logdir_spec=x:training/train/,y:training/eval/

  

    
</li>

  <li><b>Save model for export:</b>
      <br></br>
    
    python3 models/research/object_detection/exporter_main_v2.py \
    --input_type=image_tensor \
    --pipeline_config_path='pipeline_file.config' \
    --trained_checkpoint_dir=training/ \
    --output_directory=export/
    
    
</li>

<li><b>Convert to tf-lite</b>
      <br></br>
    
    python3 models/research/object_detection/export_tflite_graph_tf2.py \
    --pipeline_config_path='pipeline_file.config' \
    --trained_checkpoint_dir=export_ckpt/ \
    --output_directory=tflite/
    
    
    tflite_convert --saved_model_dir=tflite/saved_model \
    --output_file=tflite/saved_model/detect.tflite \
    --input_shapes=1,300,300,3 \
    --input_arrays=normalized_input_image_tensor \
    --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
    --inference_type=QUANTIZED_UINT8 \
    --allow_custom_ops

    
</li>


</ol>

 <br></br>
 
 
## Demo

### Video of car detecting a green light and moving forward

https://user-images.githubusercontent.com/49088702/126445878-8abe7908-83cc-4e99-9257-bcf1bf172c07.mov


### Pictures of the car
![IMG_1486](https://user-images.githubusercontent.com/49088702/126484762-095f1e8a-4132-4e52-87cc-bbc15434eeac.jpg)
![IMG_1485](https://user-images.githubusercontent.com/49088702/126484766-dd9c5119-eb30-473f-b141-357dbe83657e.jpg)
![IMG_1484](https://user-images.githubusercontent.com/49088702/126484769-a4e964d1-0ef8-4c50-941a-3ac09c1a3648.jpg)





